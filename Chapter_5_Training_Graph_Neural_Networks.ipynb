{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF8f0rJGCmVAh61kWPqfc8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl"
      ],
      "metadata": {
        "id": "lysVIOjWwmjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "This chapter discusses how to train a graph neural network for node classification, edge classification, link prediction, and graph classification for small graph(s), by message passing methods introduced in Chapter 2: Message Passing and neural network modules introduced in Chapter 3: Building GNN Modules.\n",
        "\n",
        "This chapter assumes that your graph as well as all of its node and edge features can fit into GPU; see Chapter 6: Stochastic Training on Large Graphs if they cannot.\n",
        "\n",
        "The following text assumes that the graph(s) and node/edge features are already prepared. If you plan to use the dataset DGL provides or other compatible `DGLDataset` as is described in Chapter 4: Graph Data Pipeline, you can get the graph for a single-graph dataset with something like"
      ],
      "metadata": {
        "id": "pR3iXATpwf0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "\n",
        "dataset = dgl.data.CiteseerGraphDataset()\n",
        "graph = dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2YZmgiCwfme",
        "outputId": "b14eba03-4c97-47af-a065-6a8cddd9696d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Node Classification/Regression\n",
        "\n",
        "One of the most popular and widely adopted tasks for graph neural networks is node classification, where each node in the training/validation/test set is assigned a ground truth category from a set of predefined categories. Node regression is similar, where each node in the training/validation/test set is assigned a ground truth number.\n",
        "\n",
        "### Overview\n",
        "To classify nodes, graph neural network performs message passing discussed in Chapter 2: Message Passing to utilize the nodeâ€™s own features, but also its neighboring node and edge features. Message passing can be repeated multiple rounds to incorporate information from larger range of neighborhood.\n",
        "\n",
        "### Writing neural network model\n",
        "DGL provides a few built-in graph convolution modules that can perform one round of message passing. In this guide, we choose `dgl.nn.pytorch.SAGEConv` (also available in MXNet and Tensorflow), the graph convolution module for GraphSAGE.\n",
        "\n",
        "Usually for deep learning models on graphs we need a multi-layer graph neural network, where we do multiple rounds of message passing. This can be achieved by stacking graph convolution modules as follows.\n",
        "\n"
      ],
      "metadata": {
        "id": "l-AU6S5Qwv-s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sultfEMnwPAa"
      },
      "outputs": [],
      "source": [
        "# Contruct a two-layer GNN model\n",
        "import dgl.nn as dglnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class SAGE(nn.Module):\n",
        "    def __init__(self, in_feats, hid_feats, out_feats):\n",
        "        super().__init__()\n",
        "        self.conv1 = dglnn.SAGEConv(\n",
        "            in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')\n",
        "        self.conv2 = dglnn.SAGEConv(\n",
        "            in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n",
        "\n",
        "    def forward(self, graph, inputs):\n",
        "        # inputs are features of nodes\n",
        "        h = self.conv1(graph, inputs)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(graph, h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** that you can use the model above for not only node classification, but also obtaining hidden node representations for other downstream tasks such as 5.2 Edge Classification/Regression, 5.3 Link Prediction, or 5.4 Graph Classification.\n",
        "\n",
        "For a complete list of built-in graph convolution modules, please refer to apinn.\n",
        "\n",
        "For more details in how DGL neural network modules work and how to write a custom neural network module with message passing please refer to the example in Chapter 3: Building GNN Modules."
      ],
      "metadata": {
        "id": "STIO6LsIyDej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop\n",
        "\n",
        "Training on the full graph simply involves a forward propagation of the model defined above, and computing the loss by comparing the prediction against ground truth labels on the training nodes.\n",
        "\n",
        "This section uses a DGL built-in dataset `dgl.data.CiteseerGraphDataset` to show a training loop. The node features and labels are stored on its graph instance, and the training-validation-test split are also stored on the graph as boolean masks. This is similar to what you have seen in Chapter 4: Graph Data Pipeline."
      ],
      "metadata": {
        "id": "nUBKIBnkr6tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "node_features = graph.ndata['feat']\n",
        "node_labels = graph.ndata['label']\n",
        "train_mask = graph.ndata['train_mask']\n",
        "valid_mask = graph.ndata['val_mask']\n",
        "test_mask = graph.ndata['test_mask']\n",
        "n_features = node_features.shape[1]\n",
        "n_labels = int(node_labels.max().item() + 1)"
      ],
      "metadata": {
        "id": "4gEMwvL6x5PI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is an example of evaluating your model by accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "clFTIAZMsUg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, graph, features, labels, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(graph, features)\n",
        "        logits = logits[mask]\n",
        "        labels = labels[mask]\n",
        "        _, indices = torch.max(logits, dim=1)\n",
        "        correct = torch.sum(indices == labels)\n",
        "        return correct.item() * 1.0 / len(labels)"
      ],
      "metadata": {
        "id": "MqypQFFLsFK-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then write our training loop as follows."
      ],
      "metadata": {
        "id": "c6pAph1rsXNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SAGE(in_feats=n_features, hid_feats=100, out_feats=n_labels)\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    # forward propagation by using all nodes\n",
        "    logits = model(graph, node_features)\n",
        "    # compute loss\n",
        "    loss = F.cross_entropy(logits[train_mask], node_labels[train_mask])\n",
        "    # compute validation accuracy\n",
        "    acc = evaluate(model, graph, node_features, node_labels, valid_mask)\n",
        "    # backward propagation\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    print(loss.item())\n",
        "\n",
        "    # Save model if necessary.  Omitted in this example."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYFnN5H7sY-6",
        "outputId": "2ebe63f1-f2fb-4a50-b571-f2878c83dde4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7925282716751099\n",
            "1.777905821800232\n",
            "1.7636406421661377\n",
            "1.7488237619400024\n",
            "1.7331427335739136\n",
            "1.7165350914001465\n",
            "1.699110746383667\n",
            "1.6809422969818115\n",
            "1.661994457244873\n",
            "1.6423442363739014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GraphSAGE provides an end-to-end homogeneous graph node classification example. You could see the corresponding model implementation is in the GraphSAGE class in the example with adjustable number of layers, dropout probabilities, and customizable aggregation functions and nonlinearities."
      ],
      "metadata": {
        "id": "pcxVX4Kisb8z"
      }
    }
  ]
}